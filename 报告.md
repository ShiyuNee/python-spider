## 中文

### 数据爬取

本实验对四大名著的内容进行爬取，并针对四大名著的内容展开中文文本分析，统计熵，验证齐夫定律

- 爬取网站: https://5000yan.com/
- 以水浒传的爬取为例展示爬取过程

#### 爬取界面

![image-20231021233959512](C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20231021233959512.png)

- 我们需要通过本页面，找到水浒传所有章节对应的`url`，从而获取每一个章节的信息

- 可以注意到，这里每个章节都在`class=menu-item`的`li`中，且这些项都包含在`class=panbai`的`ul`内，因此，我们对这些项进行提取，就能获得所有章节对应的`url`

- 以第一章为例，页面为

  ![image-20231021234422844](C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20231021234422844.png)

  - 可以看到，所有的正文部分都包含在`class=grap`的`div`内，因此，我们只要提取其内部所有`div`中的文字，拼接在一起即可获得全部正文

#### 爬取代码

```python
def get_book(url, out_path):
    root_url = url
    headers={'User-Agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Mobile Safari/537.36'} # chrome浏览器
    page_text=requests.get(root_url, headers=headers).content.decode()
    soup1=BeautifulSoup(page_text, 'lxml')
    res_list = []
	# 获取所有章节的url
    tag_list = soup1.find(class_='paiban').find_all(class_='menu-item')
    url_list = [item.find('a')['href'] for item in tag_list]
    for item in url_list: # 对每一章节的内容进行提取
        chapter_page = requests.get(item, headers=headers).content.decode()
        chapter_soup = BeautifulSoup(chapter_page, 'lxml')
        res = ''
        try:
            chapter_content = chapter_soup.find(class_='grap')
        except:
            raise ValueError(f'no grap in the page {item}')
        chapter_text = chapter_content.find_all('div')
        print(chapter_text)
        for div_item in chapter_text:
            res += div_item.text.strip()
        res_list.append({'text': res})
    write_jsonl(res_list, out_path)
```

- 我们使用`beautifulsoup`库，模拟`Chrome`浏览器的`header`，对每一本书的正文内容进行提取，并将结果保存到本地

### 数据清洗

- 因为文本中会有括号，其中的内容是对正文内容的拼音，以及解释。这些解释是不需要的，因此我们首先对去除括号中的内容。**注意是中文的括号**

  ```python
  def filter_cn(text):
      a = re.sub(u"\\（.*?）|\\{.*?}|\\[.*?]|\\【.*?】|\\(.*?\\)", "", text)
      return a
  ```

- 使用结巴分词，对中文语句进行分词

  ```python
  def tokenize(text):
      return jieba.cut(text)
  ```

- 删除分词后的标点符号项

  ```python
  def remove_punc(text):
      puncs = string.punctuation + "“”，。？、‘’：！；"
      new_text = ''.join([item for item in text if item not in puncs])
      return new_text
  ```

- 对中文中存在的乱码，以及数字进行去除

  ```python
  def get_cn_and_number(text):
       return re.sub(u"([^\u4e00-\u9fa5\u0030-\u0039])","",text)
  ```

整体流程代码如下所示

```python

def collect_data(data_list: list):
    voc = defaultdict(int)
    for data in data_list:
        for idx in range(len(data)):
            filtered_data = filter_cn(data[idx]['text'])
            tokenized_data = tokenize(filtered_data)
            for item in tokenized_data:
                k = remove_punc(item)
                k = get_cn_and_number(k)
                if k != '':
                    voc[k] += 1
    return voc
```

